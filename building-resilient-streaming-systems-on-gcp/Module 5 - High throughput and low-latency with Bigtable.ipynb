{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: High throughput and low-latency with Bigtable\n",
    "1. Overview\n",
    "2. What is Bigtable?\n",
    "3. Ingesting into Bigtable\n",
    "4. Designing for Bigtable\n",
    "5. Lab 4: Streaming into Bigtable\n",
    "6. Performance Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "![](img/5-1-01.png)\n",
    "![](img/5-1-02.png)\n",
    "![](img/5-1-03.png)\n",
    "![](img/5-1-04.png)\n",
    "![](img/5-1-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Bigtable?\n",
    "![](img/5-2-01.png)\n",
    "![](img/5-2-02.png)\n",
    "![](img/5-2-03.png)\n",
    "![](img/5-2-04.png)\n",
    "![](img/5-2-05.png)\n",
    "![](img/5-2-06.png)\n",
    "![](img/5-2-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingesting into Bigtable\n",
    "![](img/5-3-01.png)\n",
    "![](img/5-3-02.png)\n",
    "![](img/5-3-03.png)\n",
    "![](img/5-3-04.png)\n",
    "![](img/5-3-05.png)\n",
    "![](img/5-3-06.png)\n",
    "![](img/5-3-07.png)\n",
    "![](img/5-3-08.png)\n",
    "![](img/5-3-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Designing for Bigtable\n",
    "![](img/5-4-01.png)\n",
    "![](img/5-4-02.png)\n",
    "![](img/5-4-03.png)\n",
    "![](img/5-4-04.png)\n",
    "![](img/5-4-05.png)\n",
    "![](img/5-4-06.png)\n",
    "![](img/5-4-07.png)\n",
    "![](img/5-4-08.png)\n",
    "![](img/5-4-09.png)\n",
    "![](img/5-4-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lab 4: Streaming into Bigtable\n",
    "![ ](img/lab4-01.png)\n",
    "### Overview\n",
    "`At the time of this writing, streaming pipelines are not available in the DataFlow Python SDK. So the streaming labs are written in Java.`\n",
    "### Lab 4: Streaming Data Pipelines into Bigtable\n",
    "In this lab you will use Dataflow to collect traffic events from simulated traffic sensor data made available through Google Cloud PubSub, and write them into a Bigtable table.\n",
    "\n",
    "- Launch Dataflow pipeline to read from PubSub and write into Bigtable\n",
    "- Open an HBase shell to query the Bigtable data\n",
    "\n",
    "### Task 1: Preparation\n",
    "You will be running a sensor simulator from the training VM. In Lab 1 you manually setup the Pub/Sub components. In this lab several of those process are automated.\n",
    "\n",
    "__Open the SSH terminal and connect to the training VM__\n",
    "\n",
    "1. In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "2. Locate the line with the instance called training_vm.\n",
    "3. On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "4. In this lab you will enter CLI commands on the training_vm.<br>\n",
    "__Verify initialization is complete__\n",
    "5. The training_vm is installing software in the background. Verify that setup is complete by checking that the following directory exists. If it does not exist, wait a few minutes and try again.<br>\n",
    "`ls /training`<br>\n",
    "Wait until setup is complete before proceeding. You can verify the installation of maven with mvn -version and the JDK with java -version.<br>\n",
    "__Copy files__\n",
    "6. A repository has been downloaded to the VM. Copy the repository to your home directory.<br>\n",
    "`cp -r /training/training-data-analyst/ .`<br>\n",
    "__Set environment variables__\n",
    "7. On the training_vm SSH terminal enter the following:<br>\n",
    "`source /training/project_env.sh`<br>\n",
    "This script sets the **DEVSHELL_PROJECT_ID** and **BUCKET** environment variables<br>\n",
    "**Prepare HBase quickstart files**\n",
    "8. In the training_vm SSH terminal run the script to download and unzip the quickstart files (you will later use these to run the HBase shell.)<br>\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./install_quickstart.sh`\n",
    "\n",
    "### Task 2: Simulate traffic sensor data into Pub/Sub\n",
    "1. In the training_vm SSH terminal, start the sensor simulator. The script reads sample data from a csv file and publishes it to Pub/Sub.<br>\n",
    "`/training/sensor_magic.sh`<br>\n",
    "This command will send 1 hour of data in 1 minute. Let the script continue to run in the current terminal.<br>\n",
    "**Open a second SSH terminal and connect to the training VM**\n",
    "1. In the upper right corner of the training_vm SSH terminal, click on the gear-shaped button () and select New Connection to training-vm from the drop-down menu. A new terminal window will open.\n",
    "![ ](img/lab4-02.png)\n",
    "3. The new terminal session will not have the required environment variables. Run the following command to set them.\n",
    "4. In the new training_vm SSH terminal enter the following:<br>\n",
    "`source /training/project_env.sh`\n",
    "### Task 3: Launch Dataflow Pipeline\n",
    "1. In the second training_vm SSH terminal, navigate to the directory for this lab. Examine the script in Cloud Shell or using nano. Do not make any changes to the code.<br>\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego \n",
    "nano run_oncloud.sh`<br>\n",
    "What does the script do?\n",
    "2. The script takes 3 required arguments: project id, bucket name, classname and possibly a 4th argument: options. In this part of the lab, we will use the --bigtable option which will direct the pipeline to write into Cloud Bigtable.\n",
    "3. Run the following script to create the Bigtable instance.\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./create_cbt.sh`\n",
    "4. Run the Dataflow pipeline to read from PubSub and write into Cloud Bigtable\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./run_oncloud.sh DEVSHELL_PROJECT_ID  BUCKET CurrentConditions --bigtable`<br>\n",
    "Example successful run:<br>\n",
    "`[INFO] ------------------------------------------------------------------------\n",
    "[INFO] BUILD SUCCESS\n",
    "[INFO] ------------------------------------------------------------------------\n",
    "[INFO] Total time: 47.582 s\n",
    "[INFO] Finished at: 2018-06-08T21:25:32+00:00\n",
    "[INFO] Final Memory: 58M/213M\n",
    "[INFO] ------------------------------------------------------------------------`\n",
    "\n",
    "### Task 4: Explore the pipeline\n",
    "1. Return to the browser tab for Console. On the Navigation menu () click Dataflow and click on the new pipeline job. Confirm that the pipeline job is listed and verify that it is running without errors.\n",
    "2. Find the write:cbt step in the pipeline graph, and click on the down arrow on the right to see the writer in action. Review the Bigtable options in the step summary.\n",
    "\n",
    "### Task 5: Query Bigtable data\n",
    "1. In the second training_vm SSH terminal, run the quickstart.sh script to launch the HBase shell.\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego/quickstart\n",
    "./quickstart.sh`\n",
    "2. If the script runs successfully, you would be in an HBase shell prompt that looks something like this:\n",
    "`hbase(main):001:0>`\n",
    "3. At the HBase shell prompt, type the following query to retrieve 2 rows from your Bigtable table that was populated by the pipeline.\n",
    "`scan 'current_conditions', {'LIMIT' => 2}`\n",
    "4. Review the output. Notice each row is broken into column, timestamp, value combinations.\n",
    "5. Run another query. This time look only at the lane: speed column, limit to 10 rows, and specify rowid patterns for start and end rows to scan over.\n",
    "`scan 'current_conditions', {'LIMIT' => 10, STARTROW => '15#S#1', ENDROW => '15#S#999', COLUMN => 'lane:speed'}`\n",
    "6. Review the output. Notice that you see 10 of the column, timestamp, value combinations, all of which correspond to Highway 15. Also notice that column is restricted to lane: speed.\n",
    "7. Feel free to run other queries if you are familiar with the syntax. Once you're satisfied, ‘quit' to exit the shell.\n",
    "`quit`\n",
    "\n",
    "### Cleanup\n",
    "1. Run the script to delete your Bigtable instance<br>\n",
    "`cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./delete_cbt.sh`\n",
    "2. On your Dataflow page in your Cloud Console, click on the pipeline job name and click the ‘stop job' on the right panel.\n",
    "3. Go back to the first Cloud Shell tab with the publisher and type Ctrl-C to stop it.\n",
    "4. Go to the BigQuery console and delete the dataset demos.\n",
    "\n",
    "### Completion\n",
    "**Cleanup**<br>\n",
    "In the Cloud Platform Console, sign out of the Google account.\n",
    "\n",
    "Close the browser tab.\n",
    "\n",
    "End your lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5: Quiz\n",
    "### Question 1\n",
    "Which of the following are true about Cloud Bigtable?\n",
    "(Mark all 3 correct responses)\n",
    "- [x] **Offers very low-latency in the order of milliseconds**\n",
    "- [x] **Ideal for >1TB data**\n",
    "- [x] **Great for time-series data**\n",
    "- [ ] Support for SQL\n",
    "\n",
    "### Question 2\n",
    "True or False?\n",
    "Cloud Bigtable learns access patterns and attempts to distribute reads and storage across nodes evenly\n",
    "\n",
    "- [x] **True**\n",
    "- [ ] False\n",
    "\n",
    "### Question 3\n",
    "Which of the following can help improve performance of Bigtable?\n",
    "(Select all 3 correct responses)\n",
    "\n",
    "- [x] **Change schema to minimize data skew**\n",
    "- [x] **Clients and Bigtable are in same zone**\n",
    "- [ ] Use HDD instead of SDD\n",
    "- [x] **Add more nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
